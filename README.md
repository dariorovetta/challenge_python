
# ğŸš€ Web Scraper con Selenium, Docker y Google Cloud Run

## ğŸ“Œ DescripciÃ³n

Este proyecto implementa un **web scraper** que extrae noticias desde [Yogonet](https://www.yogonet.com/international/), realiza un post-procesamiento de datos y almacena los resultados en **Google BigQuery**.

* Se ejecuta tanto en **local** como en un **contenedor Docker**.
* EstÃ¡ desplegado como un **Cloud Run Job** en **Google Cloud**.
* Automatiza la ejecuciÃ³n de la extracciÃ³n de datos y su almacenamiento en BigQuery.
* Se proporciona un **script de despliegue (`deploy.sh`)** para gestionar automÃ¡ticamente la construcciÃ³n y despliegue del contenedor en Google Cloud Run.

## ğŸ“ **Estructura del Proyecto**

```
â”œâ”€â”€ 05_challenge_we_are_pipol_python/
â”‚   â”œâ”€â”€ script_principal.py      # CÃ³digo principal del scraper
â”‚   â”œâ”€â”€ requirements.txt         # Dependencias de Python
â”‚   â”œâ”€â”€ Dockerfile               # ConfiguraciÃ³n para Docker
â”‚   â”œâ”€â”€ deploy.sh                # Script de despliegue automatizado
â”‚   â”œâ”€â”€ cloudrun_bigquery_service.json  # Credenciales para BigQuery (NO subir a GitHub)
â”‚   â”œâ”€â”€ README.md                # DocumentaciÃ³n del proyecto
```

---

## ğŸ› ï¸ **InstalaciÃ³n y EjecuciÃ³n en Local**

### **1ï¸âƒ£ Clonar el repositorio**

```sh
git clone https://github.com/dariorovetta/challenge_python.git
cd challenge_python
```

### **2ï¸âƒ£ Crear un entorno virtual y activarlo**

```sh
python -m venv venv
source venv/bin/activate  # Mac/Linux
venv\Scripts\activate     # Windows
```

### **3ï¸âƒ£ Instalar dependencias**

```sh
pip install -r requirements.txt
```

### **4ï¸âƒ£ Ejecutar el scraper en local**

```sh
python script_principal.py
```

---

## ğŸ³ **Ejecutar con Docker**

### **1ï¸âƒ£ Construir la imagen de Docker**

```sh
docker build -t mi_scraper .
```

### **2ï¸âƒ£ Ejecutar el contenedor con credenciales**

```sh
docker run --rm \
  -v "$(pwd)/cloudrun_bigquery_service.json:/app/cloudrun_bigquery_service.json" \
  -e GOOGLE_APPLICATION_CREDENTIALS="/app/cloudrun_bigquery_service.json" \
  mi_scraper
```

ğŸ“Œ **Para Windows (CMD o PowerShell), usar ruta absoluta:**

```sh
docker run --rm -v "C:/ruta_absoluta/cloudrun_bigquery_service.json:/app/cloudrun_bigquery_service.json" \
  -e GOOGLE_APPLICATION_CREDENTIALS="/app/cloudrun_bigquery_service.json" mi_scraper
```

---

## â˜ï¸ **Despliegue Automatizado con `deploy.sh`**

### **1ï¸âƒ£ ConfiguraciÃ³n previa**

Antes de ejecutar el script, asegÃºrate de haber iniciado sesiÃ³n en Google Cloud y tener permisos suficientes:

```sh
gcloud auth login
gcloud auth configure-docker us-central1-docker.pkg.dev
```

### **2ï¸âƒ£ Ejecutar el script de despliegue**

El script `deploy.sh` se encarga de:
âœ… Construir la imagen Docker.
âœ… Subirla a **Google Artifact Registry**.
âœ… Desplegar el Job en **Google Cloud Run**.

Para ejecutarlo, simplemente usa:

```sh
bash deploy.sh
```

DespuÃ©s del despliegue, verÃ¡s el mensaje con el comando para ejecutar el Job en Cloud Run manualmente:

```sh
gcloud run jobs execute mi-scraper-job --region us-central1
```

Si deseas que el Job se ejecute automÃ¡ticamente despuÃ©s del despliegue, puedes modificar `deploy.sh` agregando esta lÃ­nea al final:

```sh
gcloud run jobs execute mi-scraper-job --region us-central1
```

---

## â˜ï¸ **Despliegue manual en Google Cloud Run** (Alternativa a `deploy.sh`)

Si prefieres hacer el proceso manualmente, estos son los pasos:

### **1ï¸âƒ£ Subir la imagen a Google Artifact Registry**

```sh
docker tag mi_scraper us-central1-docker.pkg.dev/mi-proyecto-cloudrun/scraper-repo/mi_scraper
docker push us-central1-docker.pkg.dev/mi-proyecto-cloudrun/scraper-repo/mi_scraper
```

### **2ï¸âƒ£ Desplegar como Cloud Run Job**

```sh
gcloud run jobs deploy mi-scraper-job \
  --image us-central1-docker.pkg.dev/mi-proyecto-cloudrun/scraper-repo/mi_scraper \
  --region us-central1 \
  --task-timeout=900 \
  --memory 2048Mi \
  --set-env-vars=GOOGLE_APPLICATION_CREDENTIALS="/app/cloudrun_bigquery_service.json"
```

### **3ï¸âƒ£ Ejecutar el Job en la nube**

```sh
gcloud run jobs execute mi-scraper-job --region us-central1
```

### **4ï¸âƒ£ Ver logs en Google Cloud**

```sh
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=mi-scraper-job" --limit 50
```

ğŸ”— O ver logs en: [Google Cloud Logs](https://console.cloud.google.com/logs)

---

## ğŸ“Œ **Consideraciones Finales**

âœ” **Cloud Run Job se ejecuta bajo demanda y finaliza automÃ¡ticamente.**
âœ” **Los datos son subidos a BigQuery tras cada ejecuciÃ³n.**
âœ” **Se recomienda no subir las credenciales (`cloudrun_bigquery_service.json`) a GitHub.**
âœ” **`deploy.sh` automatiza el proceso de despliegue, facilitando su ejecuciÃ³n.**
